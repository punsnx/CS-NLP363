{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-yqfdSwC-yj",
        "outputId": "18399fd5-2bf2-4b92-a952-e508b63cdf01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pythainlp in /Users/sirisuk/anaconda3/envs/nlp_py3.9/lib/python3.9/site-packages (5.0.4)\n",
            "Requirement already satisfied: requests>=2.22.0 in /Users/sirisuk/anaconda3/envs/nlp_py3.9/lib/python3.9/site-packages (from pythainlp) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sirisuk/anaconda3/envs/nlp_py3.9/lib/python3.9/site-packages (from requests>=2.22.0->pythainlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/sirisuk/anaconda3/envs/nlp_py3.9/lib/python3.9/site-packages (from requests>=2.22.0->pythainlp) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sirisuk/anaconda3/envs/nlp_py3.9/lib/python3.9/site-packages (from requests>=2.22.0->pythainlp) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/sirisuk/anaconda3/envs/nlp_py3.9/lib/python3.9/site-packages (from requests>=2.22.0->pythainlp) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install pythainlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "019Mxh_vWudq",
        "outputId": "a0d3a07c-3d5a-498a-a7f4-98605ffa8ce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-07-24 18:03:41--  https://docs.google.com/uc?export=download&id=1iqQN_PkvIpVVYYtDsW1uqP42s10wPzOu\n",
            "Resolving docs.google.com (docs.google.com)... 2404:6800:4001:801::200e, 216.58.221.206\n",
            "Connecting to docs.google.com (docs.google.com)|2404:6800:4001:801::200e|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1iqQN_PkvIpVVYYtDsW1uqP42s10wPzOu&export=download [following]\n",
            "--2024-07-24 18:03:42--  https://drive.usercontent.google.com/download?id=1iqQN_PkvIpVVYYtDsW1uqP42s10wPzOu&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 2404:6800:4001:807::2001, 142.250.199.1\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|2404:6800:4001:807::2001|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62645 (61K) [application/octet-stream]\n",
            "Saving to: ‘shopping-comment.csv’\n",
            "\n",
            "shopping-comment.cs 100%[===================>]  61.18K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-07-24 18:03:45 (851 KB/s) - ‘shopping-comment.csv’ saved [62645/62645]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1iqQN_PkvIpVVYYtDsW1uqP42s10wPzOu' -O shopping-comment.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxCWaC0AGYI2"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3CKw6JNE3nB",
        "outputId": "ad5970a0-91bd-455f-c644-facb4c2edc66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1190, 1642)\n",
            "(1642,)\n",
            "['งุด' 'ง่วง' 'ง่าย' 'จ' 'จง' 'จดจำ' 'จน' 'จนกว่า' 'จนถึง' 'จบ' 'จม'\n",
            " 'จรจัด' 'จริง' 'จริงใจ' 'จริงๆ' 'จอ' 'จอมปลอม' 'จะ' 'จัง' 'จังหวะ']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load and preprocess the data\n",
        "data = pd.read_csv('shopping-comment.csv')  # Replace 'your_dataset.csv' with the path to your dataset\n",
        "X = data['text'].astype(str)  # Text data in the 'message' column\n",
        "y = data['class']  # Labels in the 'class' column\n",
        "\n",
        "\n",
        "# Step 2: Create bag-of-words representation\n",
        "# Preprocess the data Before training the model, you need to preprocess the Thai text data to convert it into tokenized\n",
        "# and numerical features. We'll use scikit-learn's CountVectorizer to transform the text data into bag-of-word features (bow).\n",
        "\n",
        "# Tokenize Thai text\n",
        "X_tokenized = X.apply(word_tokenize, keep_whitespace=False)\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer=lambda x: x)  # Use the list of tokens as the analyzer\n",
        "X_bow = vectorizer.fit_transform(X_tokenized)# bag of word\n",
        "print(X_bow.shape) # (documents, vocab)\n",
        "\n",
        "vocab = np.array(vectorizer.get_feature_names_out())\n",
        "print(vocab.shape)\n",
        "print(vocab[250:270])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvU2KsBTKOrW",
        "outputId": "3f4bfcd1-fc57-48dc-a2d8-3a7d4a49ec42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['สินค้า', 'หมด', 'ทำไม', 'ไม่', 'แจ้ง', 'ขึ้น', 'ว่า', 'หมด', 'อะ', 'กด', 'ใส่', 'ตะกร้า', 'ไป', 'เถอะ', 'เซ็ง', 'เรย']\n",
            "  (0, 1615)\t1\n",
            "  (0, 1021)\t1\n",
            "  (0, 1069)\t2\n",
            "  (0, 483)\t1\n",
            "  (0, 1464)\t1\n",
            "  (0, 121)\t1\n",
            "  (0, 934)\t1\n",
            "  (0, 1159)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 1589)\t1\n",
            "  (0, 387)\t1\n",
            "  (0, 1611)\t1\n",
            "  (0, 1283)\t1\n",
            "  (0, 1263)\t1\n",
            "  (0, 1353)\t1\n",
            "สินค้า\n",
            "หมด\n"
          ]
        }
      ],
      "source": [
        "print(X_tokenized[2])\n",
        "print(X_bow[2])\n",
        "print(vocab[1021])\n",
        "print(vocab[1069])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZyiESEvJ6Fn",
        "outputId": "36b2ea66-b165-44e7-ddee-2637c7abdc42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8487394957983193\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the binomial logistic regression\n",
        "logreg_classifier = LogisticRegression()\n",
        "logreg_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = logreg_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV0I0HVf8sUu",
        "outputId": "d67379c8-5b5d-4298-8830-b79b7d228e47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.96      0.88       135\n",
            "           1       0.94      0.70      0.80       103\n",
            "\n",
            "    accuracy                           0.85       238\n",
            "   macro avg       0.87      0.83      0.84       238\n",
            "weighted avg       0.86      0.85      0.84       238\n",
            "\n",
            "Confusion Matrix:\n",
            "[[130   5]\n",
            " [ 31  72]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Print classification report (precision, recall, F1-score, support)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIzm3Uyu80sf",
        "outputId": "f23d18cc-9612-4666-ed39-c6bb3a86b476"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Class:  0\n",
            "0    [ของ, ไม่, ตรง, ปก, ส่ง, ช้า, แพ็ค, ไม่, ดี]\n",
            "dtype: object\n",
            "  (0, 88)\t1\n",
            "  (0, 325)\t1\n",
            "  (0, 351)\t1\n",
            "  (0, 374)\t1\n",
            "  (0, 604)\t1\n",
            "  (0, 1045)\t1\n",
            "  (0, 1494)\t1\n",
            "  (0, 1615)\t2\n"
          ]
        }
      ],
      "source": [
        "# Predict New Data: Example 1\n",
        "\n",
        "new_text = \"ของไม่ตรงปก ส่งช้า แพ็คไม่ดี\"\n",
        "#new_text = [\"สินค้าหมด ทำไมไม่แจ้ง ขึ้นว่าหมดอ่ะ\"]\n",
        "new_text = pd.Series(new_text)\n",
        "new_text_tokenized = new_text.apply(word_tokenize, keep_whitespace=False)\n",
        "new_text_bow = vectorizer.transform(new_text_tokenized)\n",
        "\n",
        "predicted_class = logreg_classifier.predict(new_text_bow)\n",
        "print(\"Predicted Class: \", predicted_class[0])\n",
        "\n",
        "print(new_text_tokenized)\n",
        "print(new_text_bow)\n",
        "#print(new_text_bow.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1wpCC7d9VHc",
        "outputId": "cd745f79-afd5-4906-d7b8-7f7dc5a0327b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Class:  1\n",
            "0    [สินค้า, ดี, มี, คุณภาพ, ชอบ, ค่ะ, แนะนำ, เลย]\n",
            "dtype: object\n",
            "  (0, 227)\t1\n",
            "  (0, 236)\t1\n",
            "  (0, 299)\t1\n",
            "  (0, 351)\t1\n",
            "  (0, 762)\t1\n",
            "  (0, 1021)\t1\n",
            "  (0, 1363)\t1\n",
            "  (0, 1477)\t1\n"
          ]
        }
      ],
      "source": [
        "# Predict New Data: Example 2\n",
        "\n",
        "#new_text = [\"ส่งรวดเร็ว แม่ค้าให้ข้อมูลครบดี\"]\n",
        "new_text = [\"สินค้าดีมีคุณภาพ ชอบค่ะ แนะนำเลย\"]\n",
        "new_text = pd.Series(new_text)\n",
        "new_text_tokenized = new_text.apply(word_tokenize, keep_whitespace=False)\n",
        "new_text_bow = vectorizer.transform(new_text_tokenized)\n",
        "\n",
        "predicted_class = logreg_classifier.predict(new_text_bow)\n",
        "print(\"Predicted Class: \", predicted_class[0])\n",
        "\n",
        "print(new_text_tokenized)\n",
        "print(new_text_bow)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYe8IIPJB_dw"
      },
      "source": [
        "# Naive Bays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFsl51sbB_pu",
        "outputId": "dbc630be-4eb1-4beb-e0a0-3b26e247c23d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1190, 1642)\n",
            "Accuracy: 0.8529411764705882\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "\n",
        "# Step 1: Load and preprocess the data\n",
        "data = pd.read_csv('shopping-comment.csv')  # Replace 'your_dataset.csv' with the path to your dataset\n",
        "X = data['text'].astype(str)  # Text data in the 'message' column\n",
        "y = data['class']  # Labels in the 'class' column\n",
        "\n",
        "# Tokenize Thai text\n",
        "X_tokenized = X.apply(word_tokenize, keep_whitespace=False)\n",
        "\n",
        "# Step 2: Create bag-of-words representation\n",
        "vectorizer2 = CountVectorizer(analyzer=lambda x: x)  # Use the list of tokens as the analyzer\n",
        "X_bow = vectorizer2.fit_transform(X_tokenized)\n",
        "print(X_bow.shape) # (documents, vocab)\n",
        "\n",
        "# Step 3: Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the binomial Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7pTCZP7aYUP",
        "outputId": "955b03f4-9d78-4d0e-b47b-60dee313f07c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Class: [0 0 1 1]\n",
            "0             [ของ, ไม่, ตรง, ปก, ส่ง, ช้า, แพ็ค, ไม่]\n",
            "1    [สินค้า, หมด, ทำไม, ไม่, แจ้ง, ขึ้น, ว่า, หมด,...\n",
            "2         [ส่ง, รวดเร็ว, แม่ค้า, ให้, ข้อมูล, ครบ, ดี]\n",
            "3    [สินค้า, ดี, ราคา, ไม่, แพง, ชอบ, ค่ะ, แนะนำ, ...\n",
            "dtype: object\n",
            "  (0, 88)\t1\n",
            "  (0, 325)\t1\n",
            "  (0, 374)\t1\n",
            "  (0, 604)\t1\n",
            "  (0, 1045)\t1\n",
            "  (0, 1494)\t1\n",
            "  (0, 1615)\t2\n",
            "  (1, 121)\t1\n",
            "  (1, 483)\t1\n",
            "  (1, 934)\t1\n",
            "  (1, 1021)\t1\n",
            "  (1, 1069)\t2\n",
            "  (1, 1196)\t1\n",
            "  (1, 1464)\t1\n",
            "  (1, 1615)\t1\n",
            "  (2, 351)\t1\n",
            "  (2, 811)\t1\n",
            "  (2, 1045)\t1\n",
            "  (2, 1594)\t1\n",
            "  (3, 236)\t1\n",
            "  (3, 299)\t1\n",
            "  (3, 351)\t1\n",
            "  (3, 844)\t1\n",
            "  (3, 1021)\t1\n",
            "  (3, 1363)\t1\n",
            "  (3, 1477)\t1\n",
            "  (3, 1493)\t1\n",
            "  (3, 1615)\t1\n"
          ]
        }
      ],
      "source": [
        "new_text = [\"ของไม่ตรงปก ส่งช้า แพ็คไม่\", \"สินค้าหมด ทำไมไม่แจ้ง ขึ้นว่าหมดอ่ะ\",\"ส่งรวดเร็ว แม่ค้าให้ข้อมูลครบดี\", \"สินค้าดีราคาไม่แพง ชอบค่ะ แนะนำเลย\"]\n",
        "new_text = pd.Series(new_text)\n",
        "new_text_tokenized = new_text.apply(word_tokenize, keep_whitespace=False)\n",
        "new_text_bow = vectorizer2.transform(new_text_tokenized)\n",
        "\n",
        "predicted_class = nb_classifier.predict(new_text_bow)\n",
        "print(\"Predicted Class:\", predicted_class)\n",
        "\n",
        "print(new_text_tokenized)\n",
        "print(new_text_bow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
